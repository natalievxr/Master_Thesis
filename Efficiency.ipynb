{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962c284b-2da0-4915-9dcb-90f391344bb0",
   "metadata": {},
   "source": [
    "## Calculating negotiation efficiency\n",
    "This notebook will calculate negotiation efficiency via the number of turns and the amount of tokens. Efficient tokenisation helps reduce the amount of computing power required for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9e5359-7d39-4eb9-a689-bff2bd351c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pandas in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from python-docx) (4.13.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp312-cp312-macosx_10_13_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-docx, tiktoken\n",
      "Successfully installed python-docx-1.2.0 tiktoken-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx tiktoken pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fffb66e6-3158-426b-905c-517272b3cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to negotiation_stats.csv\n",
      "\n",
      "Negotiation Statistics:\n",
      "           Document  User Turns  Model Turns  Total Turns  User Tokens  Model Tokens  Total Tokens\n",
      " Participant 9.docx    5.000000     5.000000     10.00000    93.000000    236.000000    329.000000\n",
      "Participant 20.docx   10.000000    10.000000     20.00000   609.000000    727.000000   1336.000000\n",
      "Participant 16.docx   13.000000    13.000000     26.00000   624.000000   1094.000000   1718.000000\n",
      " Participant 5.docx    6.000000     6.000000     12.00000   235.000000    452.000000    687.000000\n",
      " Participant 4.docx    8.000000     8.000000     16.00000   329.000000    706.000000   1035.000000\n",
      "Participant 17.docx    6.000000     6.000000     12.00000   268.000000    415.000000    683.000000\n",
      "Participant 21.docx    6.000000     6.000000     12.00000   250.000000    348.000000    598.000000\n",
      " Participant 8.docx    8.000000     8.000000     16.00000   123.000000    131.000000    254.000000\n",
      "Participant 10.docx    4.000000     4.000000      8.00000   259.000000    192.000000    451.000000\n",
      " Participant 3.docx   13.000000    13.000000     26.00000   447.000000    620.000000   1067.000000\n",
      " Participant 2.docx   12.000000    12.000000     24.00000   377.000000    701.000000   1078.000000\n",
      "Participant 11.docx   10.000000    10.000000     20.00000   265.000000    519.000000    784.000000\n",
      " Participant 1.docx    5.000000     5.000000     10.00000    83.000000     74.000000    157.000000\n",
      "Participant 12.docx    6.000000     6.000000     12.00000   249.000000    327.000000    576.000000\n",
      "Participant 13.docx    5.000000     5.000000     10.00000   642.000000    560.000000   1202.000000\n",
      "Participant 22.docx    7.000000     7.000000     14.00000    92.000000    114.000000    206.000000\n",
      "Participant 18.docx   10.000000    10.000000     20.00000   252.000000    548.000000    800.000000\n",
      " Participant 7.docx   12.000000    12.000000     24.00000   358.000000    652.000000   1010.000000\n",
      "Participant 14.docx   10.000000    10.000000     20.00000   469.000000    789.000000   1258.000000\n",
      "Participant 15.docx    7.000000     7.000000     14.00000   268.000000    384.000000    652.000000\n",
      " Participant 6.docx    7.000000     7.000000     14.00000   107.000000    107.000000    214.000000\n",
      "Participant 19.docx    4.000000     4.000000      8.00000   275.000000    321.000000    596.000000\n",
      "Participant 23.docx    7.000000     7.000000     14.00000   241.000000    380.000000    621.000000\n",
      "            AVERAGE    7.869565     7.869565     15.73913   300.652174    452.043478    752.695652\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize tokenizer \n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def analyze_negotiation(doc_path):\n",
    "    doc = Document(doc_path)\n",
    "    full_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    \n",
    "    # Split turns\n",
    "    user_turns = []\n",
    "    model_turns = []\n",
    "    turns = full_text.split('\\n')\n",
    "    \n",
    "    for turn in turns:\n",
    "        if turn.startswith(\"User:\"):\n",
    "            user_turns.append(turn.replace(\"User:\", \"\").strip())\n",
    "        elif turn.startswith(\"Model:\"):\n",
    "            model_turns.append(turn.replace(\"Model:\", \"\").strip())\n",
    "    \n",
    "    # Count turns\n",
    "    user_turn_count = len(user_turns)\n",
    "    model_turn_count = len(model_turns)\n",
    "    total_turns = user_turn_count + model_turn_count\n",
    "    \n",
    "    # Count tokens\n",
    "    user_tokens = sum(count_tokens(turn) for turn in user_turns)\n",
    "    model_tokens = sum(count_tokens(turn) for turn in model_turns)\n",
    "    total_tokens = user_tokens + model_tokens\n",
    "    \n",
    "    return {\n",
    "        \"User Turns\": user_turn_count,\n",
    "        \"Model Turns\": model_turn_count,\n",
    "        \"Total Turns\": total_turns,\n",
    "        \"User Tokens\": user_tokens,\n",
    "        \"Model Tokens\": model_tokens,\n",
    "        \"Total Tokens\": total_tokens,\n",
    "    }\n",
    "\n",
    "# Process all .docx files in the 'replacement' folder\n",
    "folder_path = \"Replacement\"\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        doc_path = os.path.join(folder_path, filename)\n",
    "        stats = analyze_negotiation(doc_path)\n",
    "        stats[\"Document\"] = filename\n",
    "        results.append(stats)\n",
    "\n",
    "# Convert to df\n",
    "df = pd.DataFrame(results)\n",
    "df = df[[\"Document\", \"User Turns\", \"Model Turns\", \"Total Turns\", \"User Tokens\", \"Model Tokens\", \"Total Tokens\"]]\n",
    "\n",
    "# Calculate averages\n",
    "averages = {\n",
    "    \"Document\": \"AVERAGE\",\n",
    "    \"User Turns\": df[\"User Turns\"].mean(),\n",
    "    \"Model Turns\": df[\"Model Turns\"].mean(),\n",
    "    \"Total Turns\": df[\"Total Turns\"].mean(),\n",
    "    \"User Tokens\": df[\"User Tokens\"].mean(),\n",
    "    \"Model Tokens\": df[\"Model Tokens\"].mean(),\n",
    "    \"Total Tokens\": df[\"Total Tokens\"].mean(),\n",
    "}\n",
    "\n",
    "# Append averages to the df\n",
    "df_averages = pd.DataFrame([averages])\n",
    "df_final = pd.concat([df, df_averages], ignore_index=True)\n",
    "\n",
    "# Save to CSV \n",
    "output_path = \"negotiation_stats.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# Print the table\n",
    "print(\"\\nNegotiation Statistics:\")\n",
    "print(df_final.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "effd5857-337a-4bd3-8440-5fb1b5dcf988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data_us.csv\n",
      "\n",
      "Summary Statistics:\n",
      "      Document  Pair  Support_Total Turns  Support_Total Tokens  Undermine_Total Turns  Undermine_Total Tokens  Participant_Total Turns  Participant_Total Tokens\n",
      "28 and 29.docx 28_29                 14.0                 562.0                    4.0                   231.0                     15.0                     517.0\n",
      "40 and 41.docx 40_41                 20.0                 883.0                   12.0                   727.0                      7.0                     261.0\n",
      "42 and 43.docx 42_43                 14.0                 807.0                   36.0                  2141.0                      7.0                     452.0\n",
      "26 and 27.docx 26_27                 22.0                1157.0                   14.0                   882.0                     14.0                     938.0\n",
      "24 and 25.docx 24_25                 10.0                 228.0                   10.0                   248.0                     12.0                     371.0\n",
      "34 and 35.docx 34_35                 20.0                1221.0                   14.0                   731.0                     16.0                     522.0\n",
      "36 and 37.docx 36_37                 14.0                 830.0                   16.0                   435.0                      9.0                     625.0\n",
      "30 and 31.docx 30_31                 10.0                 244.0                   10.0                   243.0                      7.0                     418.0\n",
      "32 and 33.docx 32_33                 14.0                 668.0                   38.0                  1647.0                      6.0                     268.0\n",
      "38 and 39.docx 38_39                  4.0                 183.0                   10.0                   464.0                      5.0                     185.0\n",
      "       AVERAGE                       14.2                 678.3                   16.4                   774.9                      9.8                     455.7\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def extract_conversation(text, role):\n",
    "    \"\"\"Extracts a specific conversation section from the document text\"\"\"\n",
    "    pattern = r\"\\d+ and AI \\(\" + role + r\"\\)(.*?)(?=\\d+ and AI \\(|$)\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def extract_participant_conversation(text, pair):\n",
    "    \"\"\"Extracts the participant-participant conversation\"\"\"\n",
    "    pattern = rf\"{pair[0]} and {pair[1]}(.*?)(?=\\d+ and AI |$)\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def analyze_conversation(text, prefix1, prefix2):\n",
    "    \"\"\"Analyzes a conversation with two roles\"\"\"\n",
    "    turns = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    role1_turns = []\n",
    "    role2_turns = []\n",
    "    \n",
    "    for turn in turns:\n",
    "        if turn.startswith(prefix1):\n",
    "            role1_turns.append(turn.replace(prefix1, \"\").strip())\n",
    "        elif turn.startswith(prefix2):\n",
    "            role2_turns.append(turn.replace(prefix2, \"\").strip())\n",
    "    \n",
    "    role1_count = len(role1_turns)\n",
    "    role2_count = len(role2_turns)\n",
    "    total_turns = role1_count + role2_count\n",
    "    \n",
    "    role1_tokens = sum(count_tokens(turn) for turn in role1_turns)\n",
    "    role2_tokens = sum(count_tokens(turn) for turn in role2_turns)\n",
    "    total_tokens = role1_tokens + role2_tokens\n",
    "    \n",
    "    return {\n",
    "        f\"{prefix1} Turns\": role1_count,\n",
    "        f\"{prefix2} Turns\": role2_count,\n",
    "        \"Total Turns\": total_turns,\n",
    "        f\"{prefix1} Tokens\": role1_tokens,\n",
    "        f\"{prefix2} Tokens\": role2_tokens,\n",
    "        \"Total Tokens\": total_tokens\n",
    "    }\n",
    "\n",
    "def process_document(doc_path):\n",
    "    \"\"\"Processes a single document and extracts all conversation metrics\"\"\"\n",
    "    doc = Document(doc_path)\n",
    "    full_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    \n",
    "    # Extract participant numbers from filename (e.g., \"24_25.docx\" -> [\"24\", \"25\"])\n",
    "    filename = os.path.basename(doc_path)\n",
    "    pair = re.findall(r\"\\d+\", filename)[:2]\n",
    "    \n",
    "    # Extract and analyze each conversation type\n",
    "    support_text = extract_conversation(full_text, \"Support\")\n",
    "    undermine_text = extract_conversation(full_text, \"Undermine\")\n",
    "    participant_text = extract_participant_conversation(full_text, pair)\n",
    "    \n",
    "    support_stats = analyze_conversation(support_text, \"User:\", \"Model:\")\n",
    "    undermine_stats = analyze_conversation(undermine_text, \"User:\", \"Model:\")\n",
    "    participant_stats = analyze_conversation(participant_text, \"HR Representative:\", \"Employee:\")\n",
    "    \n",
    "    return {\n",
    "        \"Document\": filename,\n",
    "        \"Pair\": f\"{pair[0]}_{pair[1]}\",\n",
    "        **{f\"Support_{k}\": v for k, v in support_stats.items()},\n",
    "        **{f\"Undermine_{k}\": v for k, v in undermine_stats.items()},\n",
    "        **{f\"Participant_{k}\": v for k, v in participant_stats.items()},\n",
    "    }\n",
    "\n",
    "# Process all .docx files in the 'US data' folder\n",
    "folder_path = \"US data\"\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        doc_path = os.path.join(folder_path, filename)\n",
    "        stats = process_document(doc_path)\n",
    "        results.append(stats)\n",
    "\n",
    "# Convert to df\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate averages\n",
    "averages = {\n",
    "    \"Document\": \"AVERAGE\",\n",
    "    \"Pair\": \"\",\n",
    "    \"Support_User: Turns\": df[\"Support_User: Turns\"].mean(),\n",
    "    \"Support_Model: Turns\": df[\"Support_Model: Turns\"].mean(),\n",
    "    \"Support_Total Turns\": df[\"Support_Total Turns\"].mean(),\n",
    "    \"Support_User: Tokens\": df[\"Support_User: Tokens\"].mean(),\n",
    "    \"Support_Model: Tokens\": df[\"Support_Model: Tokens\"].mean(),\n",
    "    \"Support_Total Tokens\": df[\"Support_Total Tokens\"].mean(),\n",
    "    \"Undermine_User: Turns\": df[\"Undermine_User: Turns\"].mean(),\n",
    "    \"Undermine_Model: Turns\": df[\"Undermine_Model: Turns\"].mean(),\n",
    "    \"Undermine_Total Turns\": df[\"Undermine_Total Turns\"].mean(),\n",
    "    \"Undermine_User: Tokens\": df[\"Undermine_User: Tokens\"].mean(),\n",
    "    \"Undermine_Model: Tokens\": df[\"Undermine_Model: Tokens\"].mean(),\n",
    "    \"Undermine_Total Tokens\": df[\"Undermine_Total Tokens\"].mean(),\n",
    "    \"Participant_HR Representative: Turns\": df[\"Participant_HR Representative: Turns\"].mean(),\n",
    "    \"Participant_Employee: Turns\": df[\"Participant_Employee: Turns\"].mean(),\n",
    "    \"Participant_Total Turns\": df[\"Participant_Total Turns\"].mean(),\n",
    "    \"Participant_HR Representative: Tokens\": df[\"Participant_HR Representative: Tokens\"].mean(),\n",
    "    \"Participant_Employee: Tokens\": df[\"Participant_Employee: Tokens\"].mean(),\n",
    "    \"Participant_Total Tokens\": df[\"Participant_Total Tokens\"].mean(),\n",
    "}\n",
    "\n",
    "# Append averages to the df\n",
    "df_averages = pd.DataFrame([averages])\n",
    "df_final = pd.concat([df, df_averages], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"data_us.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# Print the total table  \n",
    "summary_cols = [\n",
    "    \"Document\", \"Pair\",\n",
    "    \"Support_Total Turns\", \"Support_Total Tokens\",\n",
    "    \"Undermine_Total Turns\", \"Undermine_Total Tokens\",\n",
    "    \"Participant_Total Turns\", \"Participant_Total Tokens\"\n",
    "]\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_final[summary_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6871f2b4-d469-468e-b3f1-f0c3205fe7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/natalie/Downloads/python/anaconda3/lib/python3.12/site-packages (from python-docx) (4.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6070f41c-8992-420e-ae3b-b0b4746c6ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported results to participant_token_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from docx import Document\n",
    "import tiktoken\n",
    "\n",
    "# Tokenizer for GPT-4\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Folder with Word documents\n",
    "folder_path = \"US data\"\n",
    "\n",
    "# Output CSV file\n",
    "output_csv = \"participant_token_stats.csv\"\n",
    "\n",
    "# Helper: Count tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Helper: Extract only Participant conversation\n",
    "def extract_participant_conversation(doc_text):\n",
    "    # Find the last conversation in the doc (Participant section)\n",
    "    conversations = re.split(r'\\n\\d{2} and AI \\((Support|Undermine)\\)\\n', doc_text)\n",
    "    return conversations[-1] if conversations else doc_text\n",
    "\n",
    "# Prepare data rows for CSV\n",
    "data_rows = []\n",
    "\n",
    "# Process each .docx file\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        pair_id = filename.replace(\".docx\", \"\")\n",
    "        doc_path = os.path.join(folder_path, filename)\n",
    "        doc = Document(doc_path)\n",
    "\n",
    "        # Combine all paragraphs into one string\n",
    "        full_text = \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "        # Extract only the Participant conversation\n",
    "        participant_text = extract_participant_conversation(full_text)\n",
    "\n",
    "        # Normalize speaker labels to lowercase for consistent matching\n",
    "        participant_text = participant_text.replace(\"HR Representative:\", \"HR representative:\")\n",
    "\n",
    "        # Extract turns using case-sensitive pattern\n",
    "        turns = re.findall(\n",
    "            r\"(Employee|HR representative):\\s+(.*?)(?=\\n(?:Employee|HR representative):|\\Z)\",\n",
    "            participant_text, re.DOTALL\n",
    "        )\n",
    "\n",
    "        # Per-file stats\n",
    "        stats = {\n",
    "            \"Employee\": {\"turns\": 0, \"tokens\": 0},\n",
    "            \"HR representative\": {\"turns\": 0, \"tokens\": 0}\n",
    "        }\n",
    "\n",
    "        for speaker, utterance in turns:\n",
    "            utterance = utterance.strip()\n",
    "            if utterance:\n",
    "                token_count = count_tokens(utterance)\n",
    "                stats[speaker][\"turns\"] += 1\n",
    "                stats[speaker][\"tokens\"] += token_count\n",
    "\n",
    "        total_turns = stats[\"Employee\"][\"turns\"] + stats[\"HR representative\"][\"turns\"]\n",
    "        total_tokens = stats[\"Employee\"][\"tokens\"] + stats[\"HR representative\"][\"tokens\"]\n",
    "        avg_tokens_per_turn = total_tokens / total_turns if total_turns else 0\n",
    "\n",
    "        # Append row for this pair\n",
    "        data_rows.append([\n",
    "            pair_id,\n",
    "            stats[\"Employee\"][\"turns\"], stats[\"Employee\"][\"tokens\"],\n",
    "            stats[\"HR representative\"][\"turns\"], stats[\"HR representative\"][\"tokens\"],\n",
    "            total_turns, total_tokens, round(avg_tokens_per_turn, 2)\n",
    "        ])\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\n",
    "        \"Pair\",\n",
    "        \"Employee Turns\", \"Employee Tokens\",\n",
    "        \"HR Turns\", \"HR Tokens\",\n",
    "        \"Total Turns\", \"Total Tokens\", \"Avg Tokens per Turn\"\n",
    "    ])\n",
    "    writer.writerows(data_rows)\n",
    "\n",
    "print(f\"Exported results to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1a71af3-be04-4371-83ca-b06ef683968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averages across all participant conversations:\n",
      "Average Total Turns: 14.90\n",
      "Average Total Tokens: 739.20\n"
     ]
    }
   ],
   "source": [
    "# Compute overall averages\n",
    "num_files = len(data_rows)\n",
    "sum_total_turns = sum(row[5] for row in data_rows)   # Total Turns column\n",
    "sum_total_tokens = sum(row[6] for row in data_rows)  # Total Tokens column\n",
    "\n",
    "avg_total_turns = sum_total_turns / num_files if num_files else 0\n",
    "avg_total_tokens = sum_total_tokens / num_files if num_files else 0\n",
    "\n",
    "print(\"\\nAverages across all participant conversations:\")\n",
    "print(f\"Average Total Turns: {avg_total_turns:.2f}\")\n",
    "print(f\"Average Total Tokens: {avg_total_tokens:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
